{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from SleepClassBbox import Classifier\n",
    "from dateutil.rrule import *\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ids,\n",
    "        feat_indxs\n",
    "    ):  \n",
    "        self.feat_indxs = feat_indxs\n",
    "        self.dir = 'C:/Users/Dawson/Kaggle Projects/Detect Sleep States/sleep_institute_data/chunked_data/'\n",
    "        self.series_files = []\n",
    "        self.event_files = []\n",
    "        for id in ids:\n",
    "            for f_name in os.listdir(self.dir):\n",
    "                if f_name.startswith(id) and f_name.endswith('.npy'):\n",
    "                    self.series_files.append(f_name)\n",
    "                    self.event_files.append(f_name)\n",
    "        \n",
    "        for f_name in self.series_files[:]:\n",
    "            if 'events' in f_name:\n",
    "                self.series_files.remove(f_name)\n",
    "\n",
    "        for f_name in self.event_files[:]:\n",
    "            if 'events' not in f_name:\n",
    "                self.event_files.remove(f_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.series_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        series_data = np.load(self.dir + self.series_files[index])\n",
    "        event_data = np.load(self.dir + self.event_files[index])\n",
    "\n",
    "        # Feature indices:\n",
    "        # 0: anglez 1: enmo 2: ang_min 3:ang_max 4: ang_std 5: ang_mean\n",
    "        # 6: enmo_min 7: enmo_max 8: end_std 9: enmo_mean 10: class labels\n",
    "        x = torch.Tensor(series_data[self.feat_indxs, :]).to(torch.float32)\n",
    "        y_class = torch.Tensor(series_data[-1, :]).to(torch.float32)\n",
    "\n",
    "        y_bbox = torch.Tensor([event_data[0], event_data[1]])/17280\n",
    "\n",
    "        if torch.isnan(y_bbox).any():\n",
    "            y_bbox = torch.nan_to_num(y_bbox)\n",
    "\n",
    "        return x, y_class, y_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, dataloader, cls_criterion, bbox_criterion, optimizer, scheduler, epoch, scaler):\n",
    "    model.train()\n",
    "    data_len = len(dataloader.dataset)\n",
    "    running_cls_loss = 0\n",
    "    running_bbox_loss = 0\n",
    "    running_accuracy = 0\n",
    "    num_batches = 0\n",
    "    for batch_idx, data in tqdm(enumerate(dataloader)):\n",
    "        accel_data, cls_labels, bbox_labels = data\n",
    "\n",
    "        # Scale and clip acceleration data\n",
    "        accel_data[:, 0, :] = accel_data[:, 0, :]/100\n",
    "        accel_data[:, 1, :] = torch.clamp(accel_data[:, 1, :], max=1)\n",
    "\n",
    "        # Data augmentation\n",
    "        # Flip data along time axis\n",
    "        flip = torch.randn((1))\n",
    "        if flip.item() > 0.5:\n",
    "            accel_data = torch.flip(accel_data, (2,))\n",
    "            cls_labels = torch.flip(cls_labels, (1,))\n",
    "            bbox_labels = torch.flip(bbox_labels, (1,))\n",
    "\n",
    "        # Roll data along time axis\n",
    "        shift_size = torch.randint(0, 17280, size=(1,))\n",
    "        accel_data = torch.roll(accel_data, (shift_size[0],), 2)\n",
    "        cls_labels = torch.roll(cls_labels, (shift_size[0],), 1)\n",
    "        bbox_labels = torch.roll(bbox_labels, (shift_size[0],), 1)\n",
    "\n",
    "        accel_data, cls_labels, bbox_labels = accel_data.to(device), cls_labels.to(device), bbox_labels.to(device)\n",
    " \n",
    "        # one-hot encode labels\n",
    "        cls_labels = F.one_hot(cls_labels.to(torch.int64), num_classes=3).transpose(1, 2).to(torch.float32) # (batch, # classes, time)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        torch.autograd.set_detect_anomaly(True) \n",
    "        with torch.autocast(device_type=str(device), dtype=torch.bfloat16):\n",
    "            cls_output, bbox_output = model(accel_data) # (batch, # classes, time), (batch, 2, time)\n",
    "            cls_output = F.softmax(cls_output, dim=1)\n",
    "            bbox_output = F.sigmoid(bbox_output)\n",
    "\n",
    "            cls_loss = cls_criterion(cls_output, cls_labels)\n",
    "            bbox_loss = bbox_criterion(bbox_output, bbox_labels)\n",
    "\n",
    "            loss = bbox_loss \n",
    "            \n",
    "            # loss.backward()\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "        running_cls_loss += cls_loss.item()\n",
    "        running_bbox_loss += bbox_loss.item()\n",
    "\n",
    "        preds = torch.argmax(cls_output, dim=1)\n",
    "        arg_labels = torch.argmax(cls_labels, dim=1)\n",
    "        running_accuracy += torch.sum(preds == arg_labels)/torch.numel(preds)\n",
    "        num_batches += 1\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # Clip gradients\n",
    "        scheduler.step()\n",
    "\n",
    "        if (batch_idx+1) % 200 == 0 or batch_idx == (data_len//accel_data.shape[0])-1:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]  Class Loss: {:.6f}  BBox Loss: {:.6f}  Accuracy: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(accel_data), data_len,\n",
    "                100 * batch_idx / len(dataloader), running_cls_loss/num_batches,\n",
    "                running_bbox_loss/num_batches,  running_accuracy/num_batches\n",
    "            ))\n",
    "            running_cls_loss = 0\n",
    "            running_bbox_loss = 0\n",
    "            running_accuracy = 0\n",
    "            num_batches = 0\n",
    "    \n",
    "    return model, optimizer\n",
    "\n",
    "def validate(model, device, dataloader, cls_criterion, crt_criterion):\n",
    "    model.eval()\n",
    "    running_cls_loss = 0\n",
    "    running_bbox_loss = 0\n",
    "    running_accuracy = 0\n",
    "    for batch_idx, data in tqdm(enumerate(dataloader)):\n",
    "        accel_data, cls_labels, bbox_labels = data\n",
    "\n",
    "        # Scale and clip acceleration data\n",
    "        accel_data[:, 0, :] = accel_data[:, 0, :]/100\n",
    "        accel_data[:, 1, :] = torch.clamp(accel_data[:, 1, :], max=1)\n",
    "\n",
    "        accel_data, cls_labels, bbox_labels = accel_data.to(device), cls_labels.to(device), bbox_labels.to(device)\n",
    "\n",
    "        # one-hot encode labels\n",
    "        cls_labels = F.one_hot(cls_labels.to(torch.int64), num_classes=3).transpose(1, 2).to(torch.float32) # (batch, # classes, time)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type=str(device), dtype=torch.bfloat16):\n",
    "                cls_output, bbox_output = model(accel_data) # (batch, # classes, time)\n",
    "                cls_output = F.softmax(cls_output, dim=1)\n",
    "                bbox_output = F.sigmoid(bbox_output)\n",
    "\n",
    "                cls_loss = cls_criterion(cls_output, cls_labels)\n",
    "                crt_loss = crt_criterion(bbox_output, bbox_labels)\n",
    "\n",
    "        running_cls_loss += cls_loss.item()\n",
    "        running_bbox_loss += crt_loss.item()\n",
    "        preds = torch.argmax(cls_output, dim=1)\n",
    "        arg_labels = torch.argmax(cls_labels, dim=1)\n",
    "        running_accuracy += torch.sum(preds == arg_labels)/torch.numel(preds)\n",
    "\n",
    "    print('Validation - Class Loss: {:.6f}  Crit Loss: {:.6f}  Accuracy: {:.6f}'.format(running_cls_loss/(batch_idx+1), \n",
    "                                                                                        running_bbox_loss/(batch_idx+1),\n",
    "                                                                                        running_accuracy/(batch_idx+1)\n",
    "                                                                                        ))\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def evaluate(model, device, dataloader, mode='hard'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    crit_preds = []\n",
    "    sleep_probs = []\n",
    "    awake_probs = []\n",
    "    for batch_idx, data in tqdm(enumerate(dataloader)):\n",
    "        accel_data, cls_labels, crt_labels = data\n",
    "\n",
    "        # Scale and clip acceleration data\n",
    "        accel_data[:, 0, :] = accel_data[:, 0, :]/100\n",
    "        accel_data[:, 1, :] = torch.clamp(accel_data[:, 1, :], max=1)\n",
    "\n",
    "        accel_data = accel_data.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type=str(device), dtype=torch.bfloat16):\n",
    "                cls_output, bbox_output = model(accel_data) # (batch, # classes, time)\n",
    "                cls_output = F.softmax(cls_output, dim=1)\n",
    "                bbox_output = F.sigmoid(bbox_output)\n",
    "                \n",
    "        if mode == 'hard':\n",
    "            predictions.append(torch.squeeze(torch.argmax(cls_output, dim=1)))\n",
    "            crit_preds.append(bbox_output)\n",
    "            sleep_probs.append(torch.squeeze(cls_output[:, 1, :]))\n",
    "            awake_probs.append(torch.squeeze(cls_output[:, 0, :]))\n",
    "        else:\n",
    "            predictions.append(cls_output)\n",
    "    \n",
    "    return predictions, crit_preds, sleep_probs, awake_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validation(hparams):\n",
    "    \n",
    "    print('Initializing datasets...')\n",
    "    # Define datasets\n",
    "    unique_ids = list(set([filename.split('_')[0] for filename in os.listdir('C:/Users/Dawson/Kaggle Projects/Detect Sleep States/sleep_institute_data/chunked_data')]))\n",
    "    rand_indxs = random.sample(list(range(len(unique_ids))), len(unique_ids))\n",
    "    rand_ids = [unique_ids[indx] for indx in rand_indxs]\n",
    "    train_ids = rand_ids[0:len(rand_indxs) - 50]\n",
    "    valid_ids = rand_ids[len(rand_indxs) - 50:]\n",
    "\n",
    "    train_ds = ClassDataset(train_ids, [0,1])\n",
    "    valid_ds = ClassDataset(valid_ids, [0,1])\n",
    "    \n",
    "    print('Computing label weights...')\n",
    "    # Get label weights\n",
    "    label_weights = np.zeros((3))\n",
    "    num_examples = 0\n",
    "    for day in train_ds:\n",
    "        day_label = day[1]\n",
    "        label_weights[0] += torch.where(day_label == 0)[0].shape[0]/day_label.shape[0]\n",
    "        label_weights[1] += torch.where(day_label == 1)[0].shape[0]/day_label.shape[0]\n",
    "        label_weights[2] += torch.where(day_label == 2)[0].shape[0]/day_label.shape[0]\n",
    "        num_examples += 1\n",
    "    label_weights /= num_examples\n",
    "    label_weights = torch.Tensor(label_weights)\n",
    "    print(1/label_weights)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "    print('Training device: ' + str(device))\n",
    "\n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = DataLoader(dataset=train_ds,\n",
    "                             batch_size=hparams['batch size'],\n",
    "                             shuffle=True,\n",
    "                             **kwargs)\n",
    "\n",
    "    valid_loader = DataLoader(dataset=valid_ds,\n",
    "                             batch_size=hparams['batch size'],\n",
    "                             shuffle=False,\n",
    "                              **kwargs)\n",
    "    \n",
    "    model = Classifier(2, feat_sizes=hparams['feat sizes'], depths=hparams['depths']).to(device)\n",
    "\n",
    "    print('Number of model parameters:', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['lr'])\n",
    "    cls_criterion = nn.CrossEntropyLoss(weight=1/label_weights).to(device)\n",
    "    crt_criterion = nn.L1Loss().to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['lr'],\n",
    "                                              steps_per_epoch=int(len(train_loader)),\n",
    "                                              epochs=hparams['epochs'],\n",
    "                                              anneal_strategy='cos',\n",
    "                                              pct_start=hparams['pct start'],\n",
    "                                              div_factor=hparams['start factor'],\n",
    "                                              final_div_factor=hparams['final factor'])\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for epoch in range(hparams['epochs']):\n",
    "        trained_model, optimizer = train(model, device, train_loader, cls_criterion, crt_criterion, optimizer, scheduler, epoch, scaler)\n",
    "        validate(model, device, valid_loader, cls_criterion, crt_criterion)\n",
    "    \n",
    "    return trained_model.state_dict(), optimizer.state_dict(), hparams, valid_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ensemble of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(num_models, hparams):\n",
    "    \"\"\"\n",
    "    Train num_models models and save each model to respective files. hparams can be a list of\n",
    "    dictionaries which can be indexed with i.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_models):\n",
    "        print('Training model ' + str(i+1))\n",
    "        model_state_dict, optim_state_dict, model_hparams, valid_ds = train_and_validation(hparams)\n",
    "\n",
    "        torch.save({\n",
    "            'model_state_dict': model_state_dict,\n",
    "            'optimizer_state_dict': optim_state_dict,\n",
    "            'model_hparams': model_hparams,\n",
    "        }, './trained_models/model_' + str(i+1) + '.tar')\n",
    "\n",
    "    return valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n",
      "Initializing datasets...\n",
      "Computing label weights...\n",
      "tensor([2.1131, 4.2016, 3.4632])\n",
      "Training device: cuda\n",
      "Number of model parameters: 1381401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:52,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [3184/5734 (55%)]  Class Loss: 3.297898  BBox Loss: 0.193414  Accuracy: 0.258861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "358it [01:33,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [5712/5734 (99%)]  Class Loss: 3.293508  BBox Loss: 0.197321  Accuracy: 0.229400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:33,  3.83it/s]\n",
      "84it [00:04, 18.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Class Loss: 3.293033  Crit Loss: 0.273080  Accuracy: 0.231013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:51,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [3184/5734 (55%)]  Class Loss: 3.278401  BBox Loss: 0.216595  Accuracy: 0.255229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:32,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [5712/5734 (99%)]  Class Loss: 3.297838  BBox Loss: 0.220021  Accuracy: 0.255859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:32,  3.89it/s]\n",
      "84it [00:04, 18.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Class Loss: 3.275701  Crit Loss: 0.248090  Accuracy: 0.234130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:51,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [3184/5734 (55%)]  Class Loss: 3.296016  BBox Loss: 0.200904  Accuracy: 0.255025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:32,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [5712/5734 (99%)]  Class Loss: 3.288270  BBox Loss: 0.192077  Accuracy: 0.284429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:32,  3.89it/s]\n",
      "84it [00:04, 17.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Class Loss: 3.253759  Crit Loss: 0.233534  Accuracy: 0.275617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:51,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [3184/5734 (55%)]  Class Loss: 3.274639  BBox Loss: 0.186055  Accuracy: 0.318028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:31,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [5712/5734 (99%)]  Class Loss: 3.281418  BBox Loss: 0.182732  Accuracy: 0.316042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:31,  3.91it/s]\n",
      "84it [00:04, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Class Loss: 3.257199  Crit Loss: 0.114056  Accuracy: 0.305985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:51,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [3184/5734 (55%)]  Class Loss: 3.267097  BBox Loss: 0.181341  Accuracy: 0.312979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:31,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [5712/5734 (99%)]  Class Loss: 3.278806  BBox Loss: 0.172790  Accuracy: 0.318428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:31,  3.90it/s]\n",
      "84it [00:04, 18.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Class Loss: 3.252155  Crit Loss: 0.192987  Accuracy: 0.333763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:51,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [3184/5734 (55%)]  Class Loss: 3.237920  BBox Loss: 0.189823  Accuracy: 0.339649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:32,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [5712/5734 (99%)]  Class Loss: 3.272095  BBox Loss: 0.168936  Accuracy: 0.320405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:32,  3.90it/s]\n",
      "84it [00:04, 17.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Class Loss: 3.244370  Crit Loss: 0.251301  Accuracy: 0.363192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:51,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [3184/5734 (55%)]  Class Loss: 3.283166  BBox Loss: 0.164568  Accuracy: 0.297646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "358it [01:32,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [5712/5734 (99%)]  Class Loss: 3.247427  BBox Loss: 0.170771  Accuracy: 0.351660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:32,  3.89it/s]\n",
      "84it [00:04, 18.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Class Loss: 3.187105  Crit Loss: 0.198945  Accuracy: 0.453318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:51,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [3184/5734 (55%)]  Class Loss: 3.251933  BBox Loss: 0.167444  Accuracy: 0.338188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:31,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [5712/5734 (99%)]  Class Loss: 3.279150  BBox Loss: 0.158625  Accuracy: 0.322294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [00:04, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Class Loss: 3.263918  Crit Loss: 0.223641  Accuracy: 0.341963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:51,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [3184/5734 (55%)]  Class Loss: 3.299059  BBox Loss: 0.160855  Accuracy: 0.300627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:31,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [5712/5734 (99%)]  Class Loss: 3.288335  BBox Loss: 0.158159  Accuracy: 0.310989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:31,  3.91it/s]\n",
      "84it [00:04, 18.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Class Loss: 3.248757  Crit Loss: 0.142958  Accuracy: 0.360203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:51,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [3184/5734 (55%)]  Class Loss: 3.291539  BBox Loss: 0.154999  Accuracy: 0.306487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:31,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [5712/5734 (99%)]  Class Loss: 3.300860  BBox Loss: 0.157350  Accuracy: 0.308131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [01:31,  3.91it/s]\n",
      "84it [00:04, 17.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Class Loss: 3.233306  Crit Loss: 0.167196  Accuracy: 0.376163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "depths_list = [(8,), (9,), (10,), (11,), (12,), (13,)]\n",
    "\n",
    "hparams = {\n",
    "    'epochs': 10,\n",
    "    'batch size': 16,\n",
    "    'lr': 1e-1,\n",
    "    'pct start': 0.1,\n",
    "    'start factor': 1e2,\n",
    "    'final factor': 1e3,\n",
    "    'feat sizes': (64,),\n",
    "    'depths': (14,)\n",
    "}\n",
    "torch.cuda.empty_cache()\n",
    "eval_ds = train_ensemble(1, hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(dataloader, mode='hard'):\n",
    "    \"\"\"\n",
    "    Loads models and evaluates the given dataset. Predictions are then voted on with either 'hard' or\n",
    "    'soft' voting depending on mode selection\n",
    "    \"\"\"\n",
    "    # Various initilizations\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "    print('Predicting with ' + str(device))\n",
    "\n",
    "    model_files = os.listdir('./trained_models')\n",
    "\n",
    "    # Iterate over models and make predictions\n",
    "    model_num = 0\n",
    "    if mode == 'hard':\n",
    "        pred_list = []\n",
    "        crit_points = []\n",
    "        sleep_probs = []\n",
    "        awake_probs = []\n",
    "        for file in model_files:\n",
    "            print('Evaluating model ' + str(model_num + 1))\n",
    "            checkpoint = torch.load('./trained_models/' + file)\n",
    "            hparams = checkpoint['model_hparams']\n",
    "            model = Classifier(10, feat_sizes=hparams['feat sizes'], depths=hparams['depths']).to(device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "            predictions, crt_out, slp_probs, awk_probs = evaluate(model, device, dataloader, mode=mode)\n",
    "            pred_list.append(predictions)\n",
    "            crit_points.append(crt_out)\n",
    "            sleep_probs.append(slp_probs)\n",
    "            awake_probs.append(awk_probs)\n",
    "            model_num += 1\n",
    "            \n",
    "        return pred_list, crit_points, sleep_probs, awake_probs\n",
    "    if mode == 'soft':\n",
    "        voted_preds = []\n",
    "        crt_points = []\n",
    "        for example, data in tqdm(enumerate(dataloader)):\n",
    "            pred_sums = torch.zeros((1, 3, 17280)).to(device)\n",
    "            crt_sums = torch.zeros((1, 2, 17280)).to(device)\n",
    "            model_num = 0\n",
    "            input, label = data\n",
    "            input = input /100\n",
    "            for file in model_files:\n",
    "                checkpoint = torch.load('./trained_models/' + file)\n",
    "                hparams = checkpoint['model_hparams']\n",
    "                model = Classifier(10, feat_sizes=hparams['feat sizes'], depths=hparams['depths']).to(device)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    with torch.autocast(device_type=str(device), dtype=torch.float16):\n",
    "                        out, crt_out = model(input.to(device))\n",
    "                        out = F.softmax(out, dim=1)\n",
    "                        pred_sums += out\n",
    "                        crt_sums += crt_out\n",
    "                model_num += 1\n",
    "            \n",
    "            voted_preds.append(torch.argmax(pred_sums, dim=1))\n",
    "            crt_points.append(crt_sums)\n",
    "\n",
    "        return voted_preds, crt_points\n",
    "\n",
    "def ensemble_voting(predictions, mode='hard'):\n",
    "    \"\"\"\n",
    "    Takes a nested list of model predictions and computes voting results depending on\n",
    "    mode selection.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stack predictions from each model\n",
    "    stacked_preds = []\n",
    "    for i in range(len(predictions)):\n",
    "        stacked_preds.append(torch.stack(predictions[i], dim=0))\n",
    "\n",
    "    # Stack predictions from all models\n",
    "    stacked_preds = torch.stack(stacked_preds, dim=0).to('cpu') # (model num, num days, num steps) when mode = hard\n",
    "                                                      # (model num, num classes, num days, num steps) when mode = soft\n",
    "\n",
    "    if mode == 'hard':\n",
    "        num_votes = torch.zeros((3, stacked_preds.shape[1], stacked_preds.shape[2]))\n",
    "\n",
    "        # Awake votes\n",
    "        indxs = torch.where(stacked_preds == 0)\n",
    "        num_votes[0, indxs[1], indxs[2]] += 1\n",
    "        # Asleep votes\n",
    "        indxs = torch.where(stacked_preds == 1)\n",
    "        num_votes[1, indxs[1], indxs[2]] += 1\n",
    "        # Not wearing votes\n",
    "        indxs = torch.where(stacked_preds == 2)\n",
    "        num_votes[2, indxs[1], indxs[2]] += 1\n",
    "\n",
    "        voted_predictions = torch.argmax(num_votes, dim=0) # (num_days, num steps)\n",
    "    \n",
    "    if mode == 'soft':\n",
    "        summed_preds = torch.sum(stacked_preds, dim=0) # (num classes, num days, num steps)\n",
    "        voted_predictions = torch.argmax(summed_preds, dim=0) # (num days, num steps)\n",
    "    \n",
    "    return voted_predictions\n",
    "\n",
    "def crit_point_average(crit_points):\n",
    "    stacked_crits = []\n",
    "    for i in range(len(crit_points)):\n",
    "        stacked_crits.append(torch.stack(crit_points[i], dim=0))\n",
    "    stacked_crits = torch.stack(stacked_crits, dim=0)\n",
    "    avg_crits = torch.mean(stacked_crits, dim=0)\n",
    "    return torch.squeeze(avg_crits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define datasets\n",
    "# unique_ids = list(set([filename.split('_')[0] for filename in os.listdir('C:/Users/Dawson/Kaggle Projects/Detect Sleep States/sleep_institute_data/chunked_data')]))\n",
    "# rand_indxs = random.sample(list(range(len(unique_ids))), len(unique_ids))\n",
    "# rand_ids = [unique_ids[indx] for indx in rand_indxs]\n",
    "# eval_ids = rand_ids[len(rand_indxs) - 25:]\n",
    "\n",
    "# eval_ds = ClassDataset(eval_ids, [0,1])\n",
    "\n",
    "eval_loader = DataLoader(dataset=eval_ds,\n",
    "                        batch_size=1,\n",
    "                        shuffle=False)\n",
    "\n",
    "predictions, crit_points, sleep_probabilities, awake_probabilities = ensemble_predict(eval_loader, mode='hard')\n",
    "vote_preds = ensemble_voting(predictions, mode='hard')\n",
    "crit_points = crit_point_average(crit_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longer_list(lists):\n",
    "    \"\"\"\n",
    "    Returns the larger of two lists\n",
    "    \"\"\"\n",
    "    lengths = [len(lst) for lst in lists]\n",
    "    return lists[np.argmax(lengths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prediction(prediction):\n",
    "    \"\"\"\n",
    "    Processes a single 24 hour period of predictions to output the step numbers of 'onset', 'wakeup', or nan\n",
    "    within that day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get indices where subject is predicted to be asleep and awake\n",
    "    sleep_indxs = np.where(prediction == 1)[0]\n",
    "    awake_indxs = np.where(prediction == 0)[0]\n",
    "\n",
    "    # Return nans if awake for 24 hours\n",
    "    if (len(sleep_indxs) == 0) | (len(awake_indxs) == 0):\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # Determine duration of predicted sleeping and waking windows\n",
    "    sleep_windows = np.split(sleep_indxs, np.where(np.diff(sleep_indxs, prepend=sleep_indxs[0]-1) != 1)[0])\n",
    "    sleep_duration = [array.shape[0] for array in sleep_windows]\n",
    "\n",
    "    awake_windows = np.split(awake_indxs, np.where(np.diff(awake_indxs, prepend=awake_indxs[0]-1) != 1)[0])\n",
    "    awake_duration = [array.shape[0] for array in awake_windows]\n",
    "\n",
    "    # Ignore windows shorter than 30 minutes\n",
    "    half_hour_length = 360  # Number of steps for half an hour assuming 5 seconds per step\n",
    "    valid_indxs = [i for i in range(len(sleep_duration)) if sleep_duration[i] > half_hour_length]\n",
    "    valid_sleep_windows = [sleep_windows[valid_indxs[i]] for i in range(len(valid_indxs))]\n",
    "    valid_indxs = [i for i in range(len(awake_duration)) if awake_duration[i] > half_hour_length]\n",
    "    valid_awake_windows = [awake_windows[valid_indxs[i]] for i in range(len(valid_indxs))]\n",
    "    \n",
    "    # If significant portion of predictions have nan label return nans and no valid sleep windows\n",
    "    # nan_ratio = np.where(prediction == 2)[0].shape[0]/prediction.shape[0]\n",
    "    # if (nan_ratio >= 0.5) & (len(valid_sleep_windows)==0):\n",
    "    #     return np.nan, np.nan\n",
    "\n",
    "\n",
    "    # Return nans if no valid sleep windows are found\n",
    "    if len(valid_sleep_windows) == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # Check if any valid awake windows lie between valid sleep windows\n",
    "    chosen_windows = []\n",
    "    for i in range(len(valid_sleep_windows)-1):\n",
    "        for j in range(len(valid_awake_windows)):\n",
    "            # Condition for if valid awake window is between two valid sleep windows\n",
    "            if (valid_awake_windows[j][0] > valid_sleep_windows[i][-1]) & \\\n",
    "               (valid_awake_windows[j][-1] < valid_sleep_windows[i+1][0]):\n",
    "                # Chose larger of two valid sleep windows\n",
    "                chosen_windows.append(get_longer_list([valid_sleep_windows[i], \n",
    "                                                       valid_sleep_windows[i+1]]))\n",
    "\n",
    "    # Assign onset and wakeup steps if no valid waking windows are found between valid sleep windows\n",
    "    if len(chosen_windows) == 0:\n",
    "        onset_step = valid_sleep_windows[0][0]\n",
    "        wakeup_step = valid_sleep_windows[-1][-1]\n",
    "    else: # If valid wake windows found choose largest sleep window\n",
    "        largest_window = get_longer_list(chosen_windows)\n",
    "        onset_step = largest_window[0]\n",
    "        wakeup_step = largest_window[-1]\n",
    "\n",
    "    return onset_step, wakeup_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 7\n",
    "def plot_predictions(index):\n",
    "    print('Plotting example number ' + str(index))\n",
    "    prediction = vote_preds[index].to('cpu').numpy()\n",
    "    onset_crts = crit_points[index, 0, :].to(torch.float16).to('cpu').numpy()\n",
    "    wakeup_crts = crit_points[index, 1, :].to(torch.float16).to('cpu').numpy()\n",
    "    onset_step, wakeup_step = process_prediction(prediction)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, sharex=True, squeeze=True, figsize=(16, 8))\n",
    "    steps = np.arange(eval_ds[index][1].shape[0])\n",
    "    labels = eval_ds[index][1].numpy()\n",
    "    onset_crt_label = eval_ds[index][2][0, :].numpy()\n",
    "    wakeup_crt_label = eval_ds[index][2][1, :].numpy()\n",
    "\n",
    "    accuracy = np.sum(np.array([prediction == labels]))/prediction.shape[0]\n",
    "    print('Example accuracy = {:.4f}'.format(accuracy))\n",
    "\n",
    "    ax1.plot(steps, eval_ds[index][0][0, :]/100)\n",
    "\n",
    "    sum_slp_prob = torch.zeros_like(sleep_probabilities[0][0]).to('cpu')\n",
    "    sum_awk_prob = torch.zeros_like(sleep_probabilities[0][0]).to('cpu')\n",
    "    for i in range(len(sleep_probabilities)):\n",
    "        sum_slp_prob += sleep_probabilities[i][index].to('cpu')\n",
    "        sum_awk_prob += awake_probabilities[i][index].to('cpu')\n",
    "    avg_slp_prob = sum_slp_prob / len(sleep_probabilities)\n",
    "    avg_awk_prob = sum_awk_prob / len(awake_probabilities)\n",
    "\n",
    "\n",
    "    # ax1.plot(steps, sleep_probabilities[0][index].to('cpu'))\n",
    "    # ax1.plot(steps, sleep_probabilities[1][index].to('cpu'))\n",
    "    # ax1.plot(steps, sleep_probabilities[2][index].to('cpu'))\n",
    "    # ax1.plot(steps, sleep_probabilities[3][index].to('cpu'))\n",
    "    # ax1.plot(steps, sleep_probabilities[4][index].to('cpu'))\n",
    "    ax1.plot(steps, avg_slp_prob)\n",
    "    ax1.plot(steps, avg_awk_prob, '--')\n",
    "\n",
    "    ax1.fill_between(steps, y1=1, y2=0, where=prediction==0, facecolor='green', alpha=.5)\n",
    "    ax1.fill_between(steps, y1=1, y2=0, where=prediction==1, facecolor='red', alpha=.5)\n",
    "    ax1.fill_between(steps, y1=1, y2=0, where=prediction==2, facecolor='blue', alpha=.5)\n",
    "    ax1.fill_between(steps, y1=0, y2=-1, where=labels==0, facecolor='lime', alpha=.5)\n",
    "    ax1.fill_between(steps, y1=0, y2=-1, where=labels==1, facecolor='orangered', alpha=.5)\n",
    "    ax1.fill_between(steps, y1=0, y2=-1, where=labels==2, facecolor='blue', alpha=.5)\n",
    "    ax1.vlines(onset_step, -1, 1, colors='r', label='onset')\n",
    "    ax1.vlines(wakeup_step, -1, 1, colors='b', label='wakeup')\n",
    "    ax1.set_ylabel('anglez freq')\n",
    "\n",
    "    try:\n",
    "        print('onset diff (min): ' + str(np.abs(onset_step - steps[np.where((labels==1))[-1]][0])*5/60))\n",
    "        print('wakeup diff (min): ' + str(np.abs(wakeup_step - steps[np.where((labels==1))[-1]][-1])*5/60))\n",
    "    except:\n",
    "        print('No valid sleep window')\n",
    "\n",
    "    ax2.plot(steps, eval_ds[index][0][1, :])\n",
    "\n",
    "    # ax2.plot(steps, sleep_probabilities[0][index].to('cpu'))\n",
    "    # ax2.plot(steps, sleep_probabilities[1][index].to('cpu'))\n",
    "    # ax2.plot(steps, sleep_probabilities[2][index].to('cpu'))\n",
    "    # ax2.plot(steps, sleep_probabilities[3][index].to('cpu'))\n",
    "    # ax2.plot(steps, sleep_probabilities[4][index].to('cpu'))\n",
    "    ax2.plot(steps, avg_slp_prob)\n",
    "    ax2.plot(steps, avg_awk_prob, '--')\n",
    "\n",
    "    ax2.fill_between(steps, y1=2, y2=0, where=prediction==0, facecolor='green', alpha=.5)\n",
    "    ax2.fill_between(steps, y1=2, y2=0, where=prediction==1, facecolor='red', alpha=.5)\n",
    "    ax2.fill_between(steps, y1=2, y2=0, where=prediction==2, facecolor='blue', alpha=.5)\n",
    "    ax2.fill_between(steps, y1=3, y2=2, where=labels==0, facecolor='lime', alpha=.5)\n",
    "    ax2.fill_between(steps, y1=3, y2=2, where=labels==1, facecolor='orangered', alpha=.5)\n",
    "    ax2.fill_between(steps, y1=3, y2=2, where=labels==2, facecolor='blue', alpha=.5)\n",
    "    ax2.vlines(onset_step, 0, 3, colors='r')\n",
    "    ax2.vlines(wakeup_step, 0, 3, colors='b')\n",
    "    ax2.set_xlabel('step')\n",
    "    ax2.set_ylabel('enmo')\n",
    "\n",
    "    ax3.plot(steps, onset_crts, c='orangered')\n",
    "    ax3.plot(steps, wakeup_crts, c='lime')\n",
    "    ax3.plot(steps, onset_crt_label)\n",
    "    ax3.plot(steps, wakeup_crt_label)\n",
    "    ax3.fill_between(steps, y1=0, y2=7, where=labels==1, facecolor='orangered', alpha=.5)\n",
    "    ax3.fill_between(steps, y1=0, y2=7, where=labels==2, facecolor='blue', alpha=.5)\n",
    "\n",
    "\n",
    "    onset_crts[onset_crts < 2] = 0\n",
    "    wakeup_crts[wakeup_crts < 2] = 0\n",
    "    onset_crts[onset_crts > 10] = 0\n",
    "    wakeup_crts[wakeup_crts > 10] = 0\n",
    "    onset_cent = np.sum(onset_crts*steps)/np.sum(onset_crts)\n",
    "    wakeup_cent = np.sum(wakeup_crts*steps)/np.sum(wakeup_crts)\n",
    "\n",
    "    try:\n",
    "        print('onset cent diff (min): ' + str(np.abs(onset_cent - steps[np.where((labels==1))[-1]][0])*5/60))\n",
    "        print('wakeup cent diff (min): ' + str(np.abs(wakeup_cent - steps[np.where((labels==1))[-1]][-1])*5/60))\n",
    "    except:\n",
    "        None\n",
    "\n",
    "    ax3.vlines(onset_cent, 0, 8, colors='r', label='onset')\n",
    "    ax3.vlines(wakeup_cent, 0, 8, colors='b', label='wakeup')\n",
    "    ax3.set_ylim([0, 8])\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(index)\n",
    "index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate whole series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_series(model, device, series, mode='hard'):\n",
    "    model.eval()\n",
    "\n",
    "    # Scale and clip acceleration data\n",
    "    series[:, 0, :] = series[:, 0, :]/100\n",
    "    series[:, 1, :] = torch.clamp(series[:, 1, :], max=1)\n",
    "\n",
    "    series = series.to(device)\n",
    "    \n",
    "    with torch.autocast(device_type=str(device), dtype=torch.float16):\n",
    "        output = model(series) # (batch, # classes, time)\n",
    "        output = F.softmax(output, dim=1)\n",
    "\n",
    "    if mode == 'hard':\n",
    "        predictions = torch.squeeze(torch.argmax(output, dim=1))\n",
    "    else:\n",
    "        predictions = torch.squeeze(output)\n",
    "    \n",
    "    return [predictions]\n",
    "\n",
    "def series_predict(series, mode='hard'):\n",
    "    \"\"\"\n",
    "    Loads models and evaluates an entire series. Predictions are then voted on with either 'hard' or\n",
    "    'soft' voting depending on mode selection\n",
    "    \"\"\"\n",
    "    # Various initilizations\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "    print('Predicting with ' + str(device))\n",
    "\n",
    "    model_files = os.listdir('./trained_models')\n",
    "\n",
    "    # Iterate over models and make predictions\n",
    "    pred_list = []\n",
    "    model_num = 0\n",
    "    for file in model_files:\n",
    "        checkpoint = torch.load('./trained_models/' + file)\n",
    "        hparams = checkpoint['model_hparams']\n",
    "        model = WaveNet(2, feat_sizes=hparams['feat sizes'], depths=hparams['depths']).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        predictions = evaluate_series(model, device, series, mode=mode)\n",
    "        pred_list.append(predictions)\n",
    "        model_num += 1\n",
    "    \n",
    "    return pred_list\n",
    "\n",
    "def ensemble_voting(predictions, mode='hard'):\n",
    "    \"\"\"\n",
    "    Takes a nested list of model predictions and computes voting results depending on\n",
    "    mode selection.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stack predictions from each model\n",
    "    stacked_preds = []\n",
    "    for i in range(len(predictions)):\n",
    "        stacked_preds.append(torch.stack(predictions[i], dim=0))\n",
    "\n",
    "    # Stack predictions from all models\n",
    "    stacked_preds = torch.stack(stacked_preds, dim=0).to('cpu') # (model num, num days, num steps) when mode = hard\n",
    "                                                      # (model num, num classes, num days, num steps) when mode = soft\n",
    "\n",
    "    if mode == 'hard':\n",
    "        num_votes = torch.zeros((3, stacked_preds.shape[1], stacked_preds.shape[2]))\n",
    "\n",
    "        # Awake votes\n",
    "        indxs = torch.where(stacked_preds == 0)\n",
    "        num_votes[0, indxs[1], indxs[2]] += 1\n",
    "        # Asleep votes\n",
    "        indxs = torch.where(stacked_preds == 1)\n",
    "        num_votes[1, indxs[1], indxs[2]] += 1\n",
    "        # Not wearing votes\n",
    "        indxs = torch.where(stacked_preds == 2)\n",
    "        num_votes[2, indxs[1], indxs[2]] += 1\n",
    "\n",
    "        voted_predictions = torch.argmax(num_votes, dim=0) # (num_days, num steps)\n",
    "    \n",
    "    if mode == 'soft':\n",
    "        summed_preds = torch.sum(stacked_preds, dim=0) # (num classes, num days, num steps)\n",
    "        voted_predictions = torch.argmax(summed_preds, dim=0) # (num days, num steps)\n",
    "    \n",
    "    return voted_predictions\n",
    "\n",
    "def get_series_predictions(series_df, index):\n",
    "    \"\"\"\n",
    "    Returns the voted predictions of a series\n",
    "    \"\"\"\n",
    "    id = series_df.series_id.unique()[index]\n",
    "\n",
    "    series = torch.Tensor(np.reshape(series_df[series_df.series_id.isin([id])][['anglez', 'enmo']].to_numpy().T, (1, 2, -1)))\n",
    "\n",
    "    predictions = series_predict(series, mode='hard')\n",
    "    voted_preds = ensemble_voting(predictions, mode='hard')\n",
    "\n",
    "    return voted_preds\n",
    "\n",
    "def plot_series_preds(prediction, eval_dataset):\n",
    "    onset_step, wakeup_step =  process_prediction(prediction)\n",
    "\n",
    "    id = eval_dataset.series_id.unique()[index]\n",
    "\n",
    "    series = eval_dataset[eval_dataset.series_id.isin([id])][['anglez', 'enmo']].to_numpy().T\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, squeeze=True, figsize=(16, 8))\n",
    "    steps = np.arange(series.shape[-1])\n",
    "    # labels = eval_dataset[index][1].numpy()\n",
    "\n",
    "    # accuracy = np.sum(np.array([prediction == labels]))/prediction.shape[0]\n",
    "    # print('Example accuracy = {:.4f}'.format(accuracy))\n",
    "\n",
    "    num_days = series.shape[1]//17280+1\n",
    "    day_steps = []\n",
    "    for day in range(num_days):\n",
    "        day_steps.append(day*17280)\n",
    "\n",
    "    ax1.plot(steps, series[0, :])\n",
    "    ax1.fill_between(steps, y1=100, y2=0, where=prediction[0]==0, facecolor='green', alpha=.5)\n",
    "    ax1.fill_between(steps, y1=100, y2=0, where=prediction[0]==1, facecolor='red', alpha=.5)\n",
    "    ax1.fill_between(steps, y1=100, y2=0, where=prediction[0]==2, facecolor='blue', alpha=.5)\n",
    "    # ax1.fill_between(steps, y1=0, y2=-100, where=labels==0, facecolor='lime', alpha=.5)\n",
    "    # ax1.fill_between(steps, y1=0, y2=-100, where=labels==1, facecolor='orangered', alpha=.5)\n",
    "    # ax1.fill_between(steps, y1=0, y2=-100, where=labels==2, facecolor='blue', alpha=.5)\n",
    "    ax1.vlines(onset_step, -100, 100, colors='r', label='onset')\n",
    "    ax1.vlines(wakeup_step, -100, 100, colors='b', label='wakeup')\n",
    "    ax1.vlines(day_steps, -100, 100, colors='w', label='wakeup')\n",
    "    ax1.set_ylabel('anglez freq')\n",
    "\n",
    "    ax2.plot(steps, series[1, :])\n",
    "    ax2.fill_between(steps, y1=2, y2=0, where=prediction[0]==0, facecolor='green', alpha=.5)\n",
    "    ax2.fill_between(steps, y1=2, y2=0, where=prediction[0]==1, facecolor='red', alpha=.5)\n",
    "    ax2.fill_between(steps, y1=2, y2=0, where=prediction[0]==2, facecolor='blue', alpha=.5)\n",
    "    # ax2.fill_between(steps, y1=3, y2=2, where=labels==0, facecolor='lime', alpha=.5)\n",
    "    # ax2.fill_between(steps, y1=3, y2=2, where=labels==1, facecolor='orangered', alpha=.5)\n",
    "    # ax2.fill_between(steps, y1=3, y2=2, where=labels==2, facecolor='blue', alpha=.5)\n",
    "    ax2.vlines(onset_step, 0, 3, colors='r')\n",
    "    ax2.vlines(wakeup_step, 0, 3, colors='b')\n",
    "    ax2.vlines(day_steps, 0, 3, colors='w')\n",
    "    ax2.set_xlabel('step')\n",
    "    ax2.set_ylabel('enmo')\n",
    "    \n",
    "    # day = 24\n",
    "    # plt.xlim([17280*(day-1), 17280*(day+3)])\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "vote_preds = get_series_predictions(train_series, index)\n",
    "plot_series_preds(vote_preds, train_series)\n",
    "index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing (series evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_processing(series_df):\n",
    "    \"\"\"\n",
    "    Creates and processes the enseble predictions on an entire series. The predictions are chunked based on the noon to noon day and then the usual post-processing is performed on each chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    ids = series_df.series_id.unique()\n",
    "    event_steps = []\n",
    "    event_list = []\n",
    "    row_ids = []\n",
    "    series_ids = []\n",
    "    \n",
    "    series_num = 0\n",
    "    for id in ids:\n",
    "        print('Evaluating series ' + id)\n",
    "\n",
    "        # Organize series data and chunk indices\n",
    "        series = series_df[series_df.series_id.isin([id])]\n",
    "        series_data = torch.Tensor(np.reshape(series[['anglez', 'enmo']].to_numpy().T, (1, 2, -1)))\n",
    "        print(series.timestamp)\n",
    "        noon_steps = series.step[series.timestamp.str.contains('12:00:00')].to_list()\n",
    "        print(noon_steps)\n",
    "        chunk_indxs = [series.step.iloc[0]] + [step for step in noon_steps] + [series.step.iloc[-1]]\n",
    "        chunk_indxs = list(zip(chunk_indxs, chunk_indxs[1:]))\n",
    "        \n",
    "        # Account for case where first or last timestamp in series is at noon\n",
    "        chunk_indxs = [chunk_indxs[i] for i in range(len(chunk_indxs))\n",
    "                       if chunk_indxs[i][0] != chunk_indxs[i][1]]\n",
    "        \n",
    "        # Make predictions on series\n",
    "        predictions = series_predict(series_data, mode='hard')\n",
    "        voted_preds = ensemble_voting(predictions, mode='hard')\n",
    "        # Iterate through noon indices and process chunks\n",
    "        for i in range(len(chunk_indxs)):\n",
    "            print(voted_preds[0][chunk_indxs[i][0]:chunk_indxs[i][1]].numpy().shape[0])\n",
    "            pred_chunk = voted_preds[0][chunk_indxs[i][0]:chunk_indxs[i][1]].numpy()\n",
    "            onset, wakeup = process_prediction(pred_chunk)\n",
    "            if not np.isnan(onset):\n",
    "                event_steps.append(onset)\n",
    "                event_list.append('onset')\n",
    "                event_steps.append(wakeup)\n",
    "                event_list.append('wakeup')\n",
    "                series_ids.append(id)\n",
    "                series_ids.append(id)\n",
    "\n",
    "        series_num += 1\n",
    "        if series_num > 10:\n",
    "            break\n",
    "\n",
    "    row_ids = [int(i) for i in range(len(event_list))]\n",
    "    scores = [1.0 for _ in range(len(event_list))]\n",
    "\n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame({'row_id': row_ids,\n",
    "                                  'series_id': series_ids, \n",
    "                                  'step': event_steps, \n",
    "                                  'event': event_list, \n",
    "                                  'score': scores})\n",
    "\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = series_processing(train_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.Tensor([0,1,3,6,1,1])\n",
    "test2 = torch.Tensor([1,1,3,4,8,1])\n",
    "torch.where((test == 1) & (test2 == 1))[0].shape[0]/torch.where(test2 == 1)[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, pi, exp\n",
    "SIGMA = 90\n",
    "def gauss(mu, sigma=SIGMA):\n",
    "    # guassian distribution function\n",
    "    r = [mu - 360, mu - 300, mu - 240, mu - 180, mu - 150, mu - 120, mu - 90, mu - 60, mu - 36, mu - 12, \n",
    "         mu, mu + 12, mu + 36, mu + 60, mu + 90, mu + 120, mu + 150, mu + 180, mu + 240, mu + 300,  mu + 360]\n",
    "    return [1 / (sigma * sqrt(2*pi)) * exp(-float(x - mu)**2/(2*sigma**2)) for x in r], r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
