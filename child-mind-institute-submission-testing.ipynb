{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208c11ce",
   "metadata": {
    "papermill": {
     "duration": 0.002866,
     "end_time": "2023-10-17T01:23:29.718126",
     "exception": false,
     "start_time": "2023-10-17T01:23:29.715260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcd63e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T01:23:29.726193Z",
     "iopub.status.busy": "2023-10-17T01:23:29.725889Z",
     "iopub.status.idle": "2023-10-17T01:23:33.533729Z",
     "shell.execute_reply": "2023-10-17T01:23:33.532799Z"
    },
    "papermill": {
     "duration": 3.814086,
     "end_time": "2023-10-17T01:23:33.535759",
     "exception": false,
     "start_time": "2023-10-17T01:23:29.721673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import event_scoring\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from math import sqrt, pi, exp\n",
    "from dateutil.rrule import *\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf070da7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T01:23:33.542170Z",
     "iopub.status.busy": "2023-10-17T01:23:33.541798Z",
     "iopub.status.idle": "2023-10-17T01:23:33.722212Z",
     "shell.execute_reply": "2023-10-17T01:23:33.721398Z"
    },
    "papermill": {
     "duration": 0.18558,
     "end_time": "2023-10-17T01:23:33.724037",
     "exception": false,
     "start_time": "2023-10-17T01:23:33.538457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_series = pd.read_parquet('./sleep_institute_data/train_series.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57495bea",
   "metadata": {
    "papermill": {
     "duration": 0.002558,
     "end_time": "2023-10-17T01:23:33.730538",
     "exception": false,
     "start_time": "2023-10-17T01:23:33.727980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define model architecutre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562dca5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T01:23:33.737681Z",
     "iopub.status.busy": "2023-10-17T01:23:33.737419Z",
     "iopub.status.idle": "2023-10-17T01:23:33.766320Z",
     "shell.execute_reply": "2023-10-17T01:23:33.765516Z"
    },
    "papermill": {
     "duration": 0.034631,
     "end_time": "2023-10-17T01:23:33.767977",
     "exception": false,
     "start_time": "2023-10-17T01:23:33.733346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import torch\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Define model\n",
    "class Conv1dAutoPad(nn.Conv1d):\n",
    "    \"\"\"\n",
    "    Auto-padding convolution depending on kernel size and dilation used\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding = self.dilation[0] * (self.kernel_size[0] // 2)\n",
    "\n",
    "autoconv = partial(Conv1dAutoPad, kernel_size=3, dilation=1, bias=True)\n",
    "\n",
    "class WaveNetLayerInit(nn.Module):\n",
    "    \"\"\"\n",
    "    Initializes the WaveNet layer with a dilated convolution and identity residuals and skips\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, conv=autoconv, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        :param in_channels:\n",
    "        :param out_channels:\n",
    "        :param dilation:\n",
    "        :param conv: determines what kind of convolutional layer is used\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels, self.conv = in_channels, out_channels, conv\n",
    "\n",
    "        self.tanh_conv = nn.Sequential(OrderedDict(\n",
    "            {\n",
    "                'conv' : conv(self.in_channels, self.out_channels, *args, **kwargs),\n",
    "\n",
    "                'norm' : nn.BatchNorm1d(self.out_channels),\n",
    "\n",
    "                'drop': nn.Dropout(0.1)\n",
    "            }))\n",
    "        \n",
    "        self.sig_conv = nn.Sequential(OrderedDict(\n",
    "            {\n",
    "                'conv' : conv(self.in_channels, self.out_channels, *args, **kwargs),\n",
    "\n",
    "                'norm' : nn.BatchNorm1d(self.out_channels),\n",
    "                \n",
    "                'drop': nn.Dropout(0.1)\n",
    "            }))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "        self.skip = nn.Identity()\n",
    "\n",
    "        self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation for a single WaveNet layer. Feature maps are split after dilated convolution as described in\n",
    "        PixelCNN/RNN paper\n",
    "        :param x: input data\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "\n",
    "        if self.should_apply_mapping:\n",
    "            residual = self.shortcut(x)\n",
    "\n",
    "        x_tanh = self.tanh_conv(x)\n",
    "        x_sig = self.sig_conv(x)\n",
    "\n",
    "        # Gating activation\n",
    "        x = self.tanh(x_tanh) * self.sig(x_sig)\n",
    "\n",
    "        x = self.skip(x)\n",
    "        skip = x\n",
    "\n",
    "        x += residual\n",
    "        return x, skip\n",
    "\n",
    "    @property\n",
    "    def should_apply_mapping(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "\n",
    "\n",
    "class addResidualConnection(WaveNetLayerInit):\n",
    "    def __init__(self, in_channels, out_channels, expansion=1, conv=autoconv, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Adds the parameterized residual connection if in_channels != out_channels. Standard WaveNet maintains channel\n",
    "        number throughout the network\n",
    "        :param in_channels:\n",
    "        :param out_channels:\n",
    "        :param expansion: expansion factor if in_channels != out_channels\n",
    "        :param conv:\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.expansion = expansion\n",
    "\n",
    "        self.shortcut = nn.Sequential(OrderedDict(\n",
    "            {\n",
    "                'conv' : conv(self.in_channels, self.map_channels, kernel_size=1),\n",
    "\n",
    "                'norm' : nn.BatchNorm1d(self.map_channels),\n",
    "\n",
    "                'drop': nn.Dropout(0.1)\n",
    "            })) if self.should_apply_mapping else None\n",
    "\n",
    "\n",
    "    @property\n",
    "    def map_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "\n",
    "    @property\n",
    "    def should_apply_mapping(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "\n",
    "\n",
    "class addSkipConnection(addResidualConnection):\n",
    "    def __init__(self, in_channels, out_channels, conv=autoconv, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Adds the 1X1 convolution after the gating of the dilated convolution and returns doubled number of feature maps.\n",
    "        :param in_channels:\n",
    "        :param out_channels:\n",
    "        :param conv:\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "\n",
    "        if in_channels != 1:\n",
    "            self.skip = conv(self.in_channels, self.out_channels, kernel_size=1, dilation=1) # Expects in_channels to be half of that used by the dilated convolution\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "\n",
    "class WaveNetLayer(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, layer=addSkipConnection, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = layer(in_channels, out_channels, *args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip = self.layer(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class WaveNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, layer=WaveNetLayer, block_size=1, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructs a block of WaveNet layers of size block_size where the dilation increases by a power of 2 between\n",
    "        every layer\n",
    "        :param in_channels:\n",
    "        :param out_channels:\n",
    "        :param layer:\n",
    "        :param block_size: number of WaveNet layers toj include in the block\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            layer(in_channels, out_channels, dilation=1,*args, **kwargs),\n",
    "            *[layer(out_channels * layer.expansion, out_channels, dilation=2**(i+1), *args, **kwargs)\n",
    "              for i in range(block_size-1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips_list = []\n",
    "        for layer in self.block:\n",
    "            x, layer_skip = layer(x)\n",
    "            skips_list.append(layer_skip)\n",
    "        layer_skips = torch.stack(skips_list, dim=0) # (depth, batch, channel, time)\n",
    "        return x, layer_skips\n",
    "\n",
    "\n",
    "class WaveNetConvs(nn.Module):\n",
    "    def __init__(self, in_channels, feat_sizes=(16,32), depths=(2, 3), block=WaveNetBlock, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructs the convolutional layers of the WaveNet\n",
    "        :param in_channels:\n",
    "        :param feat_sizes: tuple of the number of feature maps per layer in each block\n",
    "        :param depths: number of layers in every block of the WaveNet\n",
    "        :param block:\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.feat_sizes, self.depths = feat_sizes, depths\n",
    "\n",
    "        self.init_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, feat_sizes[0], kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm1d(feat_sizes[0]),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.in_out_pairs = list(zip(feat_sizes, feat_sizes[1:]))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            block(feat_sizes[0], feat_sizes[0], block_size=depths[0], *args, **kwargs),\n",
    "            *[block(in_channels, out_channels, block_size=depth, *args, **kwargs)\n",
    "              for (in_channels, out_channels), depth in zip(self.in_out_pairs, depths[1:])]\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        bskip_list = []\n",
    "        x = self.init_conv(x)\n",
    "        for block in self.blocks:\n",
    "            x, block_skip = block(x)\n",
    "            bskip_list.append(block_skip)\n",
    "        block_skips = torch.stack(bskip_list, dim=0) # (block, depth, batch, channel, time)\n",
    "        return x, block_skips\n",
    "\n",
    "class Transpose12(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return x.transpose(1, 2)\n",
    "        \n",
    "class ResidualBiGRU(nn.Module):\n",
    "    def __init__(self, hidden_size, n_layers=1, bidir=True):\n",
    "        super(ResidualBiGRU, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidir\n",
    "        )\n",
    "        dir_factor = 2 if bidir else 1\n",
    "        self.fc1 = nn.Linear(\n",
    "            hidden_size * dir_factor, hidden_size * dir_factor * 2\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(hidden_size * dir_factor * 2)\n",
    "        self.fc2 = nn.Linear(hidden_size * dir_factor * 2, hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        res, new_h = self.gru(x, h)\n",
    "        # res.shape = (batch_size, sequence_size, 2*hidden_size)\n",
    "\n",
    "        res = self.fc1(res)\n",
    "        res = self.ln1(res)\n",
    "        res = nn.functional.relu(res)\n",
    "\n",
    "        res = self.fc2(res)\n",
    "        res = self.ln2(res)\n",
    "        res = nn.functional.relu(res)\n",
    "\n",
    "        # skip connection\n",
    "        res = res + x\n",
    "\n",
    "        return res, new_h\n",
    "\n",
    "class MultiResidualBiGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, out_size, n_layers, bidir=True):\n",
    "        super(MultiResidualBiGRU, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_size = out_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.fc_in = nn.Linear(input_size, hidden_size)\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "        self.res_bigrus = nn.ModuleList(\n",
    "            [\n",
    "                ResidualBiGRU(hidden_size, n_layers=1, bidir=bidir)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        # if we are at the beginning of a sequence (no hidden state)\n",
    "        if h is None:\n",
    "            # (re)initialize the hidden state\n",
    "            h = [None for _ in range(self.n_layers)]\n",
    "\n",
    "        x = self.fc_in(x)\n",
    "        x = self.ln(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        new_h = []\n",
    "        for i, res_bigru in enumerate(self.res_bigrus):\n",
    "            x, new_hi = res_bigru(x, h[i])\n",
    "            new_h.append(new_hi)\n",
    "\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x, new_h  # log probabilities + hidden states\n",
    "\n",
    "class WaveNetTail(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        \"\"\"\n",
    "        Last few layers of WaveNet where skip connections are integrated and final 1X1 convolutions occur\n",
    "        :param in_channels:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # self.tail = nn.Sequential(\n",
    "        #     nn.BatchNorm1d(in_channels),\n",
    "        #     nn.Dropout(0.1),\n",
    "        #     nn.Conv1d(in_channels, in_channels//2, kernel_size=1, padding=0),\n",
    "        #     nn.BatchNorm1d(in_channels//2),\n",
    "        #     nn.Dropout(0.1),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Conv1d(in_channels//2, 3, kernel_size=1, padding=0)\n",
    "        # )\n",
    "\n",
    "        self.gru = MultiResidualBiGRU(in_channels, in_channels*2, in_channels, 4)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(in_channels, 3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Classification\n",
    "        # x = self.tail(x)\n",
    "\n",
    "        x, h = self.gru(x.transpose(1,2))\n",
    "        x = self.drop(x)\n",
    "        x = nn.functional.silu(self.linear(x))\n",
    "        return x.transpose(1,2)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_channels, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        The completed WaveNet model\n",
    "        :param in_channels:\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extraction = WaveNetConvs(in_channels, *args, **kwargs)\n",
    "        self.tail = WaveNetTail(self.feature_extraction.feat_sizes[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skips = self.feature_extraction(x)\n",
    "        skip_sum = torch.sum(skips, dim=(0, 1))\n",
    "        x_cls = self.tail(skip_sum)\n",
    "        return x_cls\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0795241",
   "metadata": {
    "papermill": {
     "duration": 0.002348,
     "end_time": "2023-10-17T01:23:33.772731",
     "exception": false,
     "start_time": "2023-10-17T01:23:33.770383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08cfb107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T01:23:33.778948Z",
     "iopub.status.busy": "2023-10-17T01:23:33.778724Z",
     "iopub.status.idle": "2023-10-17T01:23:33.787555Z",
     "shell.execute_reply": "2023-10-17T01:23:33.786694Z"
    },
    "papermill": {
     "duration": 0.013907,
     "end_time": "2023-10-17T01:23:33.789169",
     "exception": false,
     "start_time": "2023-10-17T01:23:33.775262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, accel_data, device, mode='hard'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    accel_data = nn.functional.interpolate(accel_data, scale_factor=1/10)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=str(device), dtype=torch.float16):\n",
    "            output = model(accel_data) # (batch, # classes, time)\n",
    "            output = F.softmax(output, dim=1)\n",
    "            \n",
    "    output = nn.functional.interpolate(output, scale_factor=10)\n",
    "            \n",
    "    if mode == 'hard':\n",
    "        predictions.append(torch.squeeze(torch.argmax(output, dim=1)))\n",
    "    else:\n",
    "        predictions.append(torch.squeeze(output))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def ensemble_predict(models, data, device, mode='hard'):\n",
    "    \"\"\"\n",
    "    Loads models and evaluates the given data. Predictions are then voted on with either 'hard' or\n",
    "    'soft' voting depending on mode selection. 'SOFT' VOTING CURRENTLY UNAVAILABLE\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over models and make predictions\n",
    "    pred_list = []\n",
    "    model_num = 0\n",
    "    for model in models:\n",
    "        predictions = evaluate(model, data, device, mode=mode)\n",
    "        pred_list.append(predictions)\n",
    "        model_num += 1\n",
    "    \n",
    "    return pred_list\n",
    "\n",
    "def ensemble_voting(predictions, mode='hard'):\n",
    "    \"\"\"\n",
    "    Takes a nested list of model predictions and computes voting results depending on\n",
    "    mode selection.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stack predictions from each model\n",
    "    stacked_preds = []\n",
    "    for i in range(len(predictions)):\n",
    "        stacked_preds.append(torch.stack(predictions[i], dim=0))\n",
    "\n",
    "    # Stack predictions from all models\n",
    "    stacked_preds = torch.stack(stacked_preds, dim=0).to('cpu') # (model num, num days, num steps) when mode = hard\n",
    "                                                      # (model num, num classes, num days, num steps) when mode = soft\n",
    "\n",
    "    if mode == 'hard':\n",
    "        num_votes = torch.zeros((3, stacked_preds.shape[1], stacked_preds.shape[2]))\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "            # Awake votes\n",
    "            indxs = torch.where(stacked_preds[i, :, :] == 0)\n",
    "            num_votes[0, indxs[0], indxs[1]] += 1\n",
    "            # Asleep votes\n",
    "            indxs = torch.where(stacked_preds[i, :, :] == 1)\n",
    "            num_votes[1, indxs[0], indxs[1]] += 1\n",
    "            # Not wearing votes\n",
    "            indxs = torch.where(stacked_preds[i, :, :] == 2)\n",
    "            num_votes[2, indxs[0], indxs[1]] += 1\n",
    "\n",
    "        voted_predictions = torch.argmax(num_votes, dim=0) # (num_days, num steps)\n",
    "    \n",
    "    if mode == 'soft':\n",
    "        summed_preds = torch.sum(stacked_preds, dim=0) # (num classes, num days, num steps)\n",
    "        voted_predictions = torch.argmax(summed_preds, dim=0) # (num days, num steps)\n",
    "    \n",
    "    return voted_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b07b3d",
   "metadata": {
    "papermill": {
     "duration": 0.002561,
     "end_time": "2023-10-17T01:23:33.794116",
     "exception": false,
     "start_time": "2023-10-17T01:23:33.791555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b46e907",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T01:23:33.800412Z",
     "iopub.status.busy": "2023-10-17T01:23:33.799823Z",
     "iopub.status.idle": "2023-10-17T01:23:42.807764Z",
     "shell.execute_reply": "2023-10-17T01:23:42.806382Z"
    },
    "papermill": {
     "duration": 9.013127,
     "end_time": "2023-10-17T01:23:42.809600",
     "exception": false,
     "start_time": "2023-10-17T01:23:33.796473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [1:37:17<00:00, 21.07s/it]\n"
     ]
    }
   ],
   "source": [
    "def submission_evaluation(series_data):\n",
    "    \"\"\"\n",
    "    Formats test data into 24 hour segments and generates list of predictions voted on by\n",
    "    ensemble of models.\n",
    "    \"\"\"\n",
    "    day_length = 86400//5\n",
    "    unique_ids = series_data.series_id.unique()\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "    print('Predicting with ' + str(device))\n",
    "    \n",
    "    # Load models\n",
    "    model_files = os.listdir('./trained_models')\n",
    "    models = []\n",
    "    for i in range(len(model_files)):\n",
    "        checkpoint = torch.load('./trained_models/' + model_files[i],\n",
    "                                map_location=device)\n",
    "        hparams = checkpoint['model_hparams']\n",
    "        model = Classifier(2, feat_sizes=hparams['feat sizes'], depths=hparams['depths']).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        models.append(model)\n",
    "    \n",
    "    prediction_list = []\n",
    "    series_id_list = []\n",
    "    first_steps = []\n",
    "    for id in tqdm(unique_ids):\n",
    "        num_days = int(series_data[series_data.series_id.isin([id])].shape[0]//(86400/5)+1)\n",
    "        series_anglez = series_data.anglez[series_data.series_id.isin([id])].to_numpy()\n",
    "        series_enmo = series_data.anglez[series_data.series_id.isin([id])].diff().abs().rolling(60, center=True, min_periods=1).median().to_numpy()\n",
    "        series_enmo = series_enmo/np.max(series_enmo)\n",
    "        series_step = series_data.step[series_data.series_id.isin([id])].to_numpy()\n",
    "\n",
    "        for day in range(num_days):\n",
    "            if day < num_days - 1:\n",
    "                data = torch.Tensor(np.array([[series_anglez[day_length*day:day_length*(day+1)],\n",
    "                                               series_enmo[day_length*day:day_length*(day+1)]]])).to(device)\n",
    "                chunk_steps = np.array(series_step[day_length*day:day_length*(day+1)])\n",
    "                first_step = chunk_steps[0]\n",
    "                 # Scale data\n",
    "                data[:, 0, :] = data[:, 0, :]/90\n",
    "                # data[:, 1, :] = torch.clamp(data[:, 1, :], max=1)\n",
    "\n",
    "            if day == num_days - 1:\n",
    "                # Pad incomplete days\n",
    "                pad_length = day_length - series_enmo[day_length*day:].shape[0]\n",
    "\n",
    "                end_anglez = np.pad(series_anglez[day_length*day:], ((0, pad_length)), constant_values=(0, 0))\n",
    "                end_enmo = np.pad(series_enmo[day_length*day:], ((0, pad_length)), constant_values=(0, 0))\n",
    "\n",
    "                data = torch.Tensor(np.array([[end_anglez, end_enmo]])).to(device)\n",
    "                try:\n",
    "                    chunk_steps = np.array(series_step[day_length*day:])\n",
    "                    first_step = chunk_steps[0]\n",
    "                except:\n",
    "                    chunk_steps = np.array([0])\n",
    "                    first_step = chunk_steps[0]\n",
    "\n",
    "                # Scale data\n",
    "                data[:, 0, :] = data[:, 0, :]/90\n",
    "                # data[:, 1, :] = torch.clamp(data[:, 1, :], max=1)\n",
    "                \n",
    "            pred_list = ensemble_predict(models, data, device, mode='hard')\n",
    "            voted_preds = ensemble_voting(pred_list)\n",
    "            prediction_list.append(voted_preds[0])\n",
    "            series_id_list.append(id)\n",
    "            first_steps.append(first_step)\n",
    "\n",
    "    return prediction_list, series_id_list, first_steps\n",
    "\n",
    "preds, id_list, first_steps = submission_evaluation(test_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a740b9",
   "metadata": {
    "papermill": {
     "duration": 0.002795,
     "end_time": "2023-10-17T01:23:42.815472",
     "exception": false,
     "start_time": "2023-10-17T01:23:42.812677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Post-processing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7645c21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T01:23:42.825940Z",
     "iopub.status.busy": "2023-10-17T01:23:42.825687Z",
     "iopub.status.idle": "2023-10-17T01:23:42.853186Z",
     "shell.execute_reply": "2023-10-17T01:23:42.852230Z"
    },
    "papermill": {
     "duration": 0.035253,
     "end_time": "2023-10-17T01:23:42.855158",
     "exception": false,
     "start_time": "2023-10-17T01:23:42.819905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>step</th>\n",
       "      <th>event</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>5000</td>\n",
       "      <td>onset</td>\n",
       "      <td>0.001662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>10929</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>0.001662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>20270</td>\n",
       "      <td>onset</td>\n",
       "      <td>0.001662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>27229</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>0.001662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>40010</td>\n",
       "      <td>onset</td>\n",
       "      <td>0.001662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id     series_id   step   event     score\n",
       "0       0  038441c925bb   5000   onset  0.001662\n",
       "1       1  038441c925bb  10929  wakeup  0.001662\n",
       "2       2  038441c925bb  20270   onset  0.001662\n",
       "3       3  038441c925bb  27229  wakeup  0.001662\n",
       "4       4  038441c925bb  40010   onset  0.001662"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIGMA = 240\n",
    "def gauss(mu, sigma=SIGMA):\n",
    "    # guassian distribution function\n",
    "    r = [mu]#[mu - 720, mu - 480, mu - 360, mu - 300, mu - 240, mu - 180, mu - 150, mu - 120, mu - 90, mu - 60, mu - 36, mu - 12, \n",
    "         #mu, mu + 12, mu + 36, mu + 60, mu + 90, mu + 120, mu + 150, mu + 180, mu + 240, mu + 300,  mu + 360, mu + 480, mu + 720]\n",
    "    return [1 / (sigma * sqrt(2*pi)) * exp(-float(x - mu)**2/(2*sigma**2)) for x in r], r\n",
    "\n",
    "def get_longer_list(lists):\n",
    "    \"\"\"\n",
    "    Returns the larger of two lists\n",
    "    \"\"\"\n",
    "    lengths = [len(lst) for lst in lists]\n",
    "    return lists[np.argmax(lengths)]\n",
    "\n",
    "def process_prediction(prediction):\n",
    "    \"\"\"\n",
    "    Processes a single 24 hour period of predictions to output the step numbers of 'onset', 'wakeup', or nan\n",
    "    within that day.\n",
    "    \"\"\"\n",
    "\n",
    "    # If significant portion of predictions have nan label return nans\n",
    "    nan_ratio = np.where(prediction == 2)[0].shape[0]/prediction.shape[0]\n",
    "    if nan_ratio >= 0.5:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Get indices where subject is predicted to be asleep and awake\n",
    "    sleep_indxs = np.where(prediction == 1)[0]\n",
    "    awake_indxs = np.where(prediction == 0)[0]\n",
    "\n",
    "    # Return nans if awake for 24 hours\n",
    "    if len(sleep_indxs) == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # Determine duration of predicted sleeping and waking windows\n",
    "    sleep_windows = np.split(sleep_indxs, np.where(np.diff(sleep_indxs, prepend=sleep_indxs[0]-1) != 1)[0])\n",
    "    sleep_duration = [array.shape[0] for array in sleep_windows]\n",
    "\n",
    "    awake_windows = np.split(awake_indxs, np.where(np.diff(awake_indxs, prepend=awake_indxs[0]-1) != 1)[0])\n",
    "    awake_duration = [array.shape[0] for array in awake_windows]\n",
    "\n",
    "    # Ignore windows shorter than 30 minutes\n",
    "    half_hour_length = 360  # Number of steps for half an hour assuming 5 seconds per step\n",
    "    ignore_indxs = [i for i in range(len(sleep_duration)) if sleep_duration[i] > half_hour_length]\n",
    "    valid_sleep_windows = [sleep_windows[ignore_indxs[i]] for i in range(len(ignore_indxs))]\n",
    "    ignore_indxs = [i for i in range(len(awake_duration)) if awake_duration[i] > half_hour_length]\n",
    "    valid_awake_windows = [awake_windows[ignore_indxs[i]] for i in range(len(ignore_indxs))]\n",
    "\n",
    "    # Return nans if no valid sleep windows are found\n",
    "    if len(valid_sleep_windows) == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # Check if any valid awake windows lie between valid sleep windows\n",
    "    chosen_windows = []\n",
    "    for i in range(len(valid_sleep_windows)-1):\n",
    "        for j in range(len(valid_awake_windows)):\n",
    "            # Condition for if valid awake window is between two valid sleep windows\n",
    "            if (valid_awake_windows[j][0] > valid_sleep_windows[i][-1]) & \\\n",
    "               (valid_awake_windows[j][-1] < valid_sleep_windows[i+1][0]):\n",
    "                # Chose larger of two valid sleep windows\n",
    "                chosen_windows.append(get_longer_list([valid_sleep_windows[i], \n",
    "                                                       valid_sleep_windows[i+1]]))\n",
    "    # Assign onset and wakeup steps if no valid waking windows are found between valid sleep windows\n",
    "    if len(chosen_windows) == 0:\n",
    "        onset_step = valid_sleep_windows[0][0]\n",
    "        wakeup_step = valid_sleep_windows[-1][-1]\n",
    "    else: # If valid wake windows found choose largest sleep window\n",
    "        largest_window = get_longer_list(chosen_windows)\n",
    "        onset_step = largest_window[0]\n",
    "        wakeup_step = largest_window[-1]\n",
    "\n",
    "    return onset_step, wakeup_step\n",
    "\n",
    "def postprocessing(pred_list, id_list, first_steps):\n",
    "    \"\"\"\n",
    "    Iterates over pred_list and processes sleep state predictions to return the 'onset' and 'wakeup'\n",
    "    steps for each 24 hour period. Elements from first_steps are then added to the predicted event steps\n",
    "    to account for chunking of data into 24 hour periods when series are longer than 1 day. Returns a\n",
    "    pandas dataframe in the appropriate format.\n",
    "    \"\"\"\n",
    "\n",
    "    event_steps = []\n",
    "    event_list = []\n",
    "    row_ids = []\n",
    "    series_ids = []\n",
    "    scores = []\n",
    "    for i in range(len(pred_list)):\n",
    "        # Get day-relative steps\n",
    "        onset, wakeup = process_prediction(pred_list[i].numpy())\n",
    "        \n",
    "        # Adjust for 24 hour chunking\n",
    "        onset += first_steps[i]\n",
    "        wakeup += first_steps[i]\n",
    "        \n",
    "        if not np.isnan(onset):\n",
    "            # Assume gaussian shaped confidence about predicted onset\n",
    "            onset_gauss_scores, onset_gauss_steps = gauss(onset)\n",
    "            wakeup_gauss_scores, wakeup_gauss_steps = gauss(wakeup)\n",
    "            for j in range(len(onset_gauss_scores)):\n",
    "                event_steps.append(onset_gauss_steps[j])\n",
    "                event_list.append('onset')\n",
    "                scores.append(onset_gauss_scores[j])\n",
    "                event_steps.append(wakeup_gauss_steps[j])\n",
    "                event_list.append('wakeup')\n",
    "                scores.append(wakeup_gauss_scores[j])\n",
    "                series_ids.append(id_list[i])\n",
    "                series_ids.append(id_list[i])\n",
    "\n",
    "    row_ids = [int(i) for i in range(len(event_list))]\n",
    "\n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame({'row_id': row_ids,\n",
    "                                  'series_id': series_ids, \n",
    "                                  'step': event_steps, \n",
    "                                  'event': event_list, \n",
    "                                  'score': scores})\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "sub_df = postprocessing(preds, id_list, first_steps)\n",
    "sub_df.to_csv('./submission.csv', index=False)\n",
    "sub_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a63f746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37470238971094955"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score events\n",
    "tolerances = {\n",
    "    \"onset\" : [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "    'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]    \n",
    "}\n",
    "\n",
    "column_names = {\n",
    "    'series_id_column_name': 'series_id',\n",
    "    'time_column_name': 'step',\n",
    "    'event_column_name': 'event',\n",
    "    'score_column_name': 'score',\n",
    "}\n",
    "\n",
    "sub_df = pd.read_csv('./submission.csv')\n",
    "solution = pd.read_csv('./sleep_institute_data/train_events.csv')\n",
    "\n",
    "event_scoring.score(solution, sub_df, tolerances, **column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692083eb",
   "metadata": {},
   "source": [
    "## Prediction examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b51128",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = solution.series_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ddcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "id_list = []\n",
    "for id in unique_ids:\n",
    "    if len(solution[solution.series_id.isin([id])]) == len(sub_df[sub_df.series_id.isin([id])]):\n",
    "        test_steps = solution.step[solution.series_id.isin([id])].to_numpy()\n",
    "        sub_steps = sub_df.step[sub_df.series_id.isin([id])].to_numpy()\n",
    "        diffs.append(sub_steps - test_steps)\n",
    "        id_list.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79dc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_diffs = np.concatenate(diffs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95aaaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(stacked_diffs)\n",
    "plt.xlim([-1000,1000])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.382,
   "end_time": "2023-10-17T01:23:45.149916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-17T01:23:26.767916",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
