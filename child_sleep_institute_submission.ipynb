{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from dateutil.rrule import *\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_series = pd.read_parquet('./sleep_institute_data/train_series.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model architecutre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class Conv1dAutoPad(nn.Conv1d):\n",
    "    \"\"\"\n",
    "    Auto-padding convolution depending on kernel size and dilation used\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding = (self.dilation[0] * (self.kernel_size[0] // 2),)\n",
    "\n",
    "\n",
    "autoconv = partial(Conv1dAutoPad, kernel_size=3, dilation=1, bias=False)\n",
    "\n",
    "\n",
    "class WaveNetLayerInit(nn.Module):\n",
    "    \"\"\"\n",
    "    Initializes the WaveNet layer with a dilated convolution and identity residuals and skips\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, conv=autoconv, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        :param in_channels:\n",
    "        :param out_channels:\n",
    "        :param dilation:\n",
    "        :param conv: determines what kind of convolutional layer is used\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels, self.conv = in_channels, out_channels, conv\n",
    "\n",
    "        self.dilated_conv = nn.Sequential(OrderedDict(\n",
    "            {\n",
    "                'conv' : conv(self.in_channels, self.out_channels, *args, **kwargs),\n",
    "\n",
    "                'bn' : nn.BatchNorm1d(self.out_channels),\n",
    "\n",
    "                'drop': nn.Dropout(0.1)\n",
    "            }))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "        self.skip = nn.Identity()\n",
    "\n",
    "        self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation for a single WaveNet layer. Feature maps are split after dilated convolution as described in\n",
    "        PixelCNN/RNN paper\n",
    "        :param x: input data\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "\n",
    "        if self.should_apply_mapping:\n",
    "            residual = self.shortcut(x)\n",
    "\n",
    "        x = self.dilated_conv(x)\n",
    "\n",
    "        # Gating activation\n",
    "        x = self.tanh(x) * self.sig(x)\n",
    "\n",
    "        x = self.skip(x)\n",
    "        skip = x\n",
    "\n",
    "        x += residual\n",
    "        return x, skip\n",
    "\n",
    "    @property\n",
    "    def should_apply_mapping(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "\n",
    "\n",
    "class addResidualConnection(WaveNetLayerInit):\n",
    "    def __init__(self, in_channels, out_channels, expansion=1, conv=autoconv, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Adds the parameterized residual connection if in_channels != out_channels. Standard WaveNet maintains channel\n",
    "        number throughout the network\n",
    "        :param in_channels:\n",
    "        :param out_channels:\n",
    "        :param expansion: expansion factor if in_channels != out_channels\n",
    "        :param conv:\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.expansion = expansion\n",
    "\n",
    "        self.shortcut = nn.Sequential(OrderedDict(\n",
    "            {\n",
    "                'conv' : conv(self.in_channels, self.map_channels, kernel_size=1),\n",
    "\n",
    "                'bn' : nn.BatchNorm1d(self.map_channels),\n",
    "\n",
    "                'drop': nn.Dropout(0.1)\n",
    "            })) if self.should_apply_mapping else None\n",
    "\n",
    "\n",
    "    @property\n",
    "    def map_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "\n",
    "    @property\n",
    "    def should_apply_mapping(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "\n",
    "\n",
    "class addSkipConnection(addResidualConnection):\n",
    "    def __init__(self, in_channels, out_channels, conv=autoconv, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Adds the 1X1 convolution after the gating of the dilated convolution and returns doubled number of feature maps.\n",
    "        :param in_channels:\n",
    "        :param out_channels:\n",
    "        :param conv:\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "\n",
    "        if in_channels != 1:\n",
    "            self.skip = conv(self.out_channels, self.out_channels, kernel_size=1, dilation=1) \n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "\n",
    "class WaveNetLayer(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, layer=addSkipConnection, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = layer(in_channels, out_channels, *args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip = self.layer(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class WaveNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, layer=WaveNetLayer, block_size=1, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructs a block of WaveNet layers of size block_size where the dilation increases by a power of 2 between\n",
    "        every layer\n",
    "        :param in_channels:\n",
    "        :param out_channels:\n",
    "        :param layer:\n",
    "        :param block_size: number of WaveNet layers toj include in the block\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            layer(in_channels, out_channels, dilation=1,*args, **kwargs),\n",
    "            *[layer(out_channels * layer.expansion, out_channels, dilation=2**(i+1), *args, **kwargs)\n",
    "              for i in range(block_size-1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer_skips = []\n",
    "        for layer in self.block:\n",
    "            x, layer_skip = layer(x)\n",
    "            layer_skips.append(layer_skip)\n",
    "        skips = torch.cat(layer_skips, 0)\n",
    "        return x, skips\n",
    "\n",
    "\n",
    "class WaveNetConvs(nn.Module):\n",
    "    def __init__(self, in_channels, feat_sizes=(16,32), depths=(2, 3), block=WaveNetBlock, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructs the convolutional layers of the WaveNet\n",
    "        :param in_channels:\n",
    "        :param feat_sizes: tuple of the number of feature maps per layer in each block\n",
    "        :param depths: number of layers in every block of the WaveNet\n",
    "        :param block:\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.feat_sizes, self.depths = feat_sizes, depths\n",
    "\n",
    "        self.init_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, feat_sizes[0], kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm1d(feat_sizes[0]),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.in_out_pairs = list(zip(feat_sizes, feat_sizes[1:]))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            block(feat_sizes[0], feat_sizes[0], block_size=depths[0], *args, **kwargs),\n",
    "            *[block(in_channels, out_channels, block_size=depth, *args, **kwargs)\n",
    "              for (in_channels, out_channels), depth in zip(self.in_out_pairs, depths[1:])]\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        block_skips = []\n",
    "        x = self.init_conv(x)\n",
    "        for block in self.blocks:\n",
    "            x, block_skip = block(x)\n",
    "            block_skips.append(block_skip)\n",
    "        skips = torch.cat(block_skips, 0)\n",
    "        return x, skips\n",
    "\n",
    "\n",
    "class WaveNetTail(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        \"\"\"\n",
    "        Last few layers of WaveNet where skip connections are integrated and final 1X1 convolutions occur\n",
    "        :param in_channels:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, in_channels//2, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm1d(in_channels//2),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_channels//2, 3, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm1d(3),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(3, 3, kernel_size=7, padding=3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tail(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, in_channels, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        The completed WaveNet model\n",
    "        :param in_channels:\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extraction = WaveNetConvs(in_channels, *args, **kwargs)\n",
    "        self.tail = WaveNetTail(self.feature_extraction.feat_sizes[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skips = self.feature_extraction(x)\n",
    "        skip_sum = torch.sum(skips, dim=0) \n",
    "        x += torch.relu(skip_sum)\n",
    "        x = self.tail(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, accel_data, mode='hard'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    output = model(accel_data)  # (batch, # classes, time)\n",
    "    output = F.softmax(output, dim=1)\n",
    "\n",
    "    if mode == 'hard':\n",
    "        predictions.append(torch.squeeze(torch.argmax(output, dim=1)))\n",
    "    else:\n",
    "        predictions.append(torch.squeeze(output))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def ensemble_predict(models, data, mode='hard'):\n",
    "    \"\"\"\n",
    "    Loads models and evaluates the given data. Predictions are then voted on with either 'hard' or\n",
    "    'soft' voting depending on mode selection. 'SOFT' VOTING CURRENTLY UNAVAILABLE\n",
    "    \"\"\"\n",
    "    # Iterate over models and make predictions\n",
    "    pred_list = []\n",
    "    model_num = 0\n",
    "    for model in models:\n",
    "        predictions = evaluate(model, data, mode=mode)\n",
    "        pred_list.append(predictions)\n",
    "        model_num += 1\n",
    "    \n",
    "    return pred_list\n",
    "\n",
    "def ensemble_voting(predictions, mode='hard'):\n",
    "    \"\"\"\n",
    "    Takes a nested list of model predictions and computes voting results depending on\n",
    "    mode selection.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stack predictions from each model\n",
    "    stacked_preds = []\n",
    "    for i in range(len(predictions)):\n",
    "        stacked_preds.append(torch.stack(predictions[i], dim=0))\n",
    "\n",
    "    # Stack predictions from all models\n",
    "    stacked_preds = torch.stack(stacked_preds, dim=0).to('cpu') # (model num, num days, num steps) when mode = hard\n",
    "                                                      # (model num, num classes, num days, num steps) when mode = soft\n",
    "\n",
    "    if mode == 'hard':\n",
    "        num_votes = torch.zeros((3, stacked_preds.shape[1], stacked_preds.shape[2]))\n",
    "\n",
    "        # Awake votes\n",
    "        indxs = torch.where(stacked_preds == 0)\n",
    "        num_votes[0, indxs[1], indxs[2]] += 1\n",
    "        # Asleep votes\n",
    "        indxs = torch.where(stacked_preds == 1)\n",
    "        num_votes[1, indxs[1], indxs[2]] += 1\n",
    "        # Not wearing votes\n",
    "        indxs = torch.where(stacked_preds == 2)\n",
    "        num_votes[2, indxs[1], indxs[2]] += 1\n",
    "\n",
    "        voted_predictions = torch.argmax(num_votes, dim=0) # (num_days, num steps)\n",
    "    \n",
    "    if mode == 'soft':\n",
    "        summed_preds = torch.sum(stacked_preds, dim=0) # (num classes, num days, num steps)\n",
    "        voted_predictions = torch.argmax(summed_preds, dim=0) # (num days, num steps)\n",
    "    \n",
    "    return voted_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [1:18:43<00:00, 17.05s/it]\n"
     ]
    }
   ],
   "source": [
    "def submission_evaluation(series_data):\n",
    "    \"\"\"\n",
    "    Formats test data into 24 hour segments and generates list of predictions voted on by\n",
    "    ensemble of models.\n",
    "    \"\"\"\n",
    "    day_length = 86400//5\n",
    "    unique_ids = series_data.series_id.unique()\n",
    "\n",
    "    use_mps = torch.backends.mps.is_available()\n",
    "    device = torch.device('mps' if use_cuda else 'cpu')\n",
    "    print('Predicting with ' + str(device))\n",
    "    \n",
    "    # Load models\n",
    "    model_files = os.listdir('./trained_models')\n",
    "    models = []\n",
    "    for i in range(len(model_files)):\n",
    "        checkpoint = torch.load('./trained_models/' + model_files[i],\n",
    "                                map_location=device)\n",
    "        hparams = checkpoint['model_hparams']\n",
    "        model = WaveNet(2, feat_sizes=hparams['feat sizes'], depths=hparams['depths']).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        models.append(model)\n",
    "    \n",
    "    prediction_list = []\n",
    "    series_id_list = []\n",
    "    first_steps = []\n",
    "    for id in tqdm(unique_ids):\n",
    "        num_days = int(series_data[series_data.series_id.isin([id])].shape[0]//(86400/5)+1)\n",
    "        series_anglez = series_data.anglez[series_data.series_id.isin([id])].to_numpy()\n",
    "        series_enmo = series_data.enmo[series_data.series_id.isin([id])].to_numpy()\n",
    "        series_step = series_data.step[series_data.series_id.isin([id])].to_numpy()\n",
    "\n",
    "        for day in range(num_days):\n",
    "            if day < num_days - 1:\n",
    "                data = torch.Tensor(np.array([[series_anglez[day_length*day:day_length*(day+1)],\n",
    "                                               series_enmo[day_length*day:day_length*(day+1)]]])).to(device)\n",
    "                chunk_steps = np.array(series_step[day_length*day:day_length*(day+1)])\n",
    "                first_step = chunk_steps[0]\n",
    "\n",
    "            if day == num_days - 1:\n",
    "                # Pad incomplete days\n",
    "                pad_length = day_length - series_enmo[day_length*day:].shape[0]\n",
    "\n",
    "                end_anglez = np.pad(series_anglez[day_length*day:], ((0, pad_length)), constant_values=(0, 0))\n",
    "                end_enmo = np.pad(series_enmo[day_length*day:], ((0, pad_length)), constant_values=(0, 0))\n",
    "\n",
    "                data = torch.Tensor(np.array([[end_anglez, end_enmo]])).to(device)\n",
    "                try:\n",
    "                    chunk_steps = np.array(series_step[day_length*day:])\n",
    "                    first_step = chunk_steps[0]\n",
    "                except:\n",
    "                    chunk_steps = np.array([0])\n",
    "                    first_step = chunk_steps[0]\n",
    "                \n",
    "            pred_list = ensemble_predict(models, data, mode='hard')\n",
    "            voted_preds = ensemble_voting(pred_list)\n",
    "            prediction_list.append(voted_preds[0])\n",
    "            series_id_list.append(id)\n",
    "            first_steps.append(first_step)\n",
    "\n",
    "    return prediction_list, series_id_list, first_steps\n",
    "\n",
    "preds, id_list, first_steps = submission_evaluation(test_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longer_list(lists):\n",
    "    \"\"\"\n",
    "    Returns the larger of two lists\n",
    "    \"\"\"\n",
    "    lengths = [len(lst) for lst in lists]\n",
    "    return lists[np.argmax(lengths)]\n",
    "\n",
    "def process_prediction(prediction):\n",
    "    \"\"\"\n",
    "    Processes a single 24 hour period of predictions to output the step numbers of 'onset', 'wakeup', or nan\n",
    "    within that day.\n",
    "    \"\"\"\n",
    "\n",
    "    # If significant portion of predictions have nan label return nans\n",
    "    nan_ratio = np.sum(prediction[prediction == 2])/prediction.shape[0]\n",
    "    if nan_ratio >= 0.5:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Get indices where subject is predicted to be asleep and awake\n",
    "    sleep_indxs = np.where(prediction == 1)[0]\n",
    "    awake_indxs = np.where(prediction == 0)[0]\n",
    "\n",
    "    # Return nans if awake for 24 hours\n",
    "    if len(sleep_indxs) == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # Determine duration of predicted sleeping and waking windows\n",
    "    sleep_windows = np.split(sleep_indxs, np.where(np.diff(sleep_indxs, prepend=sleep_indxs[0]-1) != 1)[0])\n",
    "    sleep_duration = [array.shape[0] for array in sleep_windows]\n",
    "\n",
    "    awake_windows = np.split(awake_indxs, np.where(np.diff(awake_indxs, prepend=awake_indxs[0]-1) != 1)[0])\n",
    "    awake_duration = [array.shape[0] for array in awake_windows]\n",
    "\n",
    "    # Ignore windows shorter than 30 minutes\n",
    "    half_hour_length = 360  # Number of steps for half an hour assuming 5 seconds per step\n",
    "    ignore_indxs = [i for i in range(len(sleep_duration)) if sleep_duration[i] > half_hour_length]\n",
    "    valid_sleep_windows = [sleep_windows[ignore_indxs[i]] for i in range(len(ignore_indxs))]\n",
    "    ignore_indxs = [i for i in range(len(awake_duration)) if awake_duration[i] > half_hour_length]\n",
    "    valid_awake_windows = [awake_windows[ignore_indxs[i]] for i in range(len(ignore_indxs))]\n",
    "\n",
    "    # Return nans if no valid sleep windows are found\n",
    "    if len(valid_sleep_windows) == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # Check if any valid awake windows lie between valid sleep windows\n",
    "    chosen_windows = []\n",
    "    for i in range(len(valid_sleep_windows)-1):\n",
    "        for j in range(len(valid_awake_windows)):\n",
    "            # Condition for if valid awake window is between two valid sleep windows\n",
    "            if (valid_awake_windows[j][0] > valid_sleep_windows[i][-1]) & \\\n",
    "               (valid_awake_windows[j][-1] < valid_sleep_windows[i+1][0]):\n",
    "                # Chose larger of two valid sleep windows\n",
    "                chosen_windows.append(get_longer_list([valid_sleep_windows[i], \n",
    "                                                       valid_sleep_windows[i+1]]))\n",
    "    # Assign onset and wakeup steps if no valid waking windows are found between valid sleep windows\n",
    "    if len(chosen_windows) == 0:\n",
    "        onset_step = valid_sleep_windows[0][0]\n",
    "        wakeup_step = valid_sleep_windows[-1][-1]\n",
    "    else: # If valid wake windows found choose largest sleep window\n",
    "        largest_window = get_longer_list(chosen_windows)\n",
    "        onset_step = largest_window[0]\n",
    "        wakeup_step = largest_window[-1]\n",
    "\n",
    "    return onset_step, wakeup_step\n",
    "\n",
    "def postprocessing(pred_list, id_list, first_steps):\n",
    "    \"\"\"\n",
    "    Iterates over pred_list and processes sleep state predictions to return the 'onset' and 'wakeup'\n",
    "    steps for each 24 hour period. Elements from first_steps are then added to the predicted event steps\n",
    "    to account for chunking of data into 24 hour periods when series are longer than 1 day. Returns a\n",
    "    pandas dataframe in the appropriate format.\n",
    "    \"\"\"\n",
    "\n",
    "    event_steps = []\n",
    "    event_list = []\n",
    "    row_ids = []\n",
    "    series_ids = []\n",
    "    for i in range(len(pred_list)):\n",
    "        # Get day-relative steps\n",
    "        onset, wakeup = process_prediction(pred_list[i].numpy())\n",
    "        \n",
    "        # Adjust for 24 hour chunking\n",
    "        onset += first_steps[i]\n",
    "        wakeup += first_steps[i]\n",
    "\n",
    "        if not np.isnan(onset):\n",
    "            event_steps.append(onset)\n",
    "            event_list.append('onset')\n",
    "            event_steps.append(wakeup)\n",
    "            event_list.append('wakeup')\n",
    "            series_ids.append(id_list[i])\n",
    "            series_ids.append(id_list[i])\n",
    "\n",
    "    row_ids = [int(i) for i in range(len(event_list))]\n",
    "    scores = [1.0 for _ in range(len(event_list))]\n",
    "\n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame({'row_id': row_ids,\n",
    "                                  'series_id': series_ids, \n",
    "                                  'step': event_steps, \n",
    "                                  'event': event_list, \n",
    "                                  'score': scores})\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "sub_df = postprocessing(preds, id_list, first_steps)\n",
    "sub_df.to_csv('./submission.csv', index=False)\n",
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
